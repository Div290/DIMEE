# ğŸš€ DIMEE: Distributed Inference on Mobile, Edge, and Cloud â€“ An Early Exit Approach  

This repository contains the official implementation of **DIMEE: Distributed Inference on Mobile, Edge, and Cloud** â€“ a novel **Early Exit Approach** to optimize inference efficiency across resource-constrained environments.  

---

## ğŸ“‹ Table of Contents  
- [âœ¨ Introduction](#-introduction)  
- [ğŸ›  Requirements](#-requirements)  
- [ğŸ“– Training](#-training)  
- [âš¡ Inference](#-inference)  
- [ğŸ“Š Datasets](#-datasets)  
- [ğŸ™ Acknowledgements](#-acknowledgements)  

---

## âœ¨ Introduction  
Deep Neural Networks (DNNs) demand high computational power, making deployment on edge and mobile devices challenging. **DIMEE** leverages **Early Exit strategies** to distribute inference dynamically across mobile, edge, and cloud environments, reducing latency and computational cost.  

---

## ğŸ›  Requirements  
Ensure you have the following dependencies installed before running the code:  

```bash
pip install torch==1.4.0 transformers==2.5.1
```

## ğŸ“– Training
Fine-tune a pre-trained language model and train the internal classifiers using the command:

```bash
bash finetune_(al)bert.sh
```

## âš¡ Inference
Run inference using the following command:

```bash
bash patience_infer_(al)bert.sh
```

## ğŸ“Š Datasets
We use benchmark GLUE datasets, available at [GLUE Datasets](https://gluebenchmark.com/).


## ğŸ™ Acknowledgements
We extend our gratitude to the authors of [PABEE](https://github.com/JetRunner/PABEE/tree/master) for making their work publicly available.
